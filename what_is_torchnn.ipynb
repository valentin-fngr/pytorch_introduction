{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f04f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bceb8",
   "metadata": {},
   "source": [
    "### Downloading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00eeb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a869ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (PATH / FILENAME).exists(): \n",
    "    # download it \n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    # write it inside folder\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f7506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2754c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d226f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77bdd373d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8klEQVR4nO3df6jVdZ7H8ddrbfojxzI39iZOrWOEUdE6i9nSyjYRTj8o7FYMIzQ0JDl/JDSwyIb7xxSLIVu6rBSDDtXYMus0UJHFMNVm5S6BdDMrs21qoxjlphtmmv1a9b1/3K9xp+75nOs53/PD+34+4HDO+b7P93zffPHl99f53o8jQgAmvj/rdQMAuoOwA0kQdiAJwg4kQdiBJE7o5sJsc+of6LCI8FjT29qy277C9lu237F9ezvfBaCz3Op1dtuTJP1B0gJJOyW9JGlRROwozMOWHeiwTmzZ50l6JyLejYgvJf1G0sI2vg9AB7UT9hmS/jjq/c5q2p+wvcT2kO2hNpYFoE0dP0EXEeskrZPYjQd6qZ0t+y5JZ4x6/51qGoA+1E7YX5J0tu3v2j5R0o8kbaynLQB1a3k3PiIO2V4q6SlJkyQ9EBFv1NYZgFq1fOmtpYVxzA50XEd+VAPg+EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEi0P2Yzjw6RJk4r1U045paPLX7p0acPaSSedVJx39uzZxfqtt95arN9zzz0Na4sWLSrO+/nnnxfrK1euLNbvvPPOYr0X2gq77fckHZB0WNKhiJhbR1MA6lfHlv3SiPiwhu8B0EEcswNJtBv2kPS07ZdtLxnrA7aX2B6yPdTmsgC0od3d+PkRscv2X0h6xvZ/R8Tm0R+IiHWS1kmS7WhzeQBa1NaWPSJ2Vc97JD0maV4dTQGoX8thtz3Z9pSjryX9QNL2uhoDUK92duMHJD1m++j3/HtE/L6WriaYM888s1g/8cQTi/WLL764WJ8/f37D2tSpU4vzXn/99cV6L+3cubNYX7NmTbE+ODjYsHbgwIHivK+++mqx/sILLxTr/ajlsEfEu5L+qsZeAHQQl96AJAg7kARhB5Ig7EAShB1IwhHd+1HbRP0F3Zw5c4r1TZs2Feudvs20Xx05cqRYv/nmm4v1Tz75pOVlDw8PF+sfffRRsf7WW2+1vOxOiwiPNZ0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2GkybNq1Y37JlS7E+a9asOtupVbPe9+3bV6xfeumlDWtffvllcd6svz9oF9fZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJhmyuwd69e4v1ZcuWFetXX311sf7KK68U683+pHLJtm3bivUFCxYU6wcPHizWzzvvvIa12267rTgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72PnDyyScX682GF167dm3D2uLFi4vz3njjjcX6hg0binX0n5bvZ7f9gO09trePmjbN9jO2366eT62zWQD1G89u/K8kXfG1abdLejYizpb0bPUeQB9rGvaI2Czp678HXShpffV6vaRr620LQN1a/W38QEQcHSzrA0kDjT5oe4mkJS0uB0BN2r4RJiKidOItItZJWidxgg7opVYvve22PV2Squc99bUEoBNaDftGSTdVr2+S9Hg97QDolKa78bY3SPq+pNNs75T0c0krJf3W9mJJ70v6YSebnOj279/f1vwff/xxy/PecsstxfrDDz9crDcbYx39o2nYI2JRg9JlNfcCoIP4uSyQBGEHkiDsQBKEHUiCsANJcIvrBDB58uSGtSeeeKI47yWXXFKsX3nllcX6008/Xayj+xiyGUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BHfWWWcV61u3bi3W9+3bV6w/99xzxfrQ0FDD2n333Vect5v/NicSrrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ09ucHCwWH/wwQeL9SlTprS87OXLlxfrDz30ULE+PDxcrGfFdXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILr7Cg6//zzi/XVq1cX65dd1vpgv2vXri3WV6xYUazv2rWr5WUfz1q+zm77Adt7bG8fNe0O27tsb6seV9XZLID6jWc3/leSrhhj+r9ExJzq8bt62wJQt6Zhj4jNkvZ2oRcAHdTOCbqltl+rdvNPbfQh20tsD9lu/MfIAHRcq2H/haSzJM2RNCxpVaMPRsS6iJgbEXNbXBaAGrQU9ojYHRGHI+KIpF9KmldvWwDq1lLYbU8f9XZQ0vZGnwXQH5peZ7e9QdL3JZ0mabekn1fv50gKSe9J+mlENL25mOvsE8/UqVOL9WuuuaZhrdm98vaYl4u/smnTpmJ9wYIFxfpE1eg6+wnjmHHRGJPvb7sjAF3Fz2WBJAg7kARhB5Ig7EAShB1Igltc0TNffPFFsX7CCeWLRYcOHSrWL7/88oa1559/vjjv8Yw/JQ0kR9iBJAg7kARhB5Ig7EAShB1IgrADSTS96w25XXDBBcX6DTfcUKxfeOGFDWvNrqM3s2PHjmJ98+bNbX3/RMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BDd79uxifenSpcX6ddddV6yffvrpx9zTeB0+fLhYHx4u//XyI0eO1NnOcY8tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX240Cza9mLFo010O6IZtfRZ86c2UpLtRgaGirWV6xYUaxv3LixznYmvKZbdttn2H7O9g7bb9i+rZo+zfYztt+unk/tfLsAWjWe3fhDkv4+Is6V9DeSbrV9rqTbJT0bEWdLerZ6D6BPNQ17RAxHxNbq9QFJb0qaIWmhpPXVx9ZLurZDPQKowTEds9ueKel7krZIGoiIoz9O/kDSQIN5lkha0kaPAGow7rPxtr8t6RFJP4uI/aNrMTI65JiDNkbEuoiYGxFz2+oUQFvGFXbb39JI0H8dEY9Wk3fbnl7Vp0va05kWAdSh6W68bUu6X9KbEbF6VGmjpJskrayeH+9IhxPAwMCYRzhfOffcc4v1e++9t1g/55xzjrmnumzZsqVYv/vuuxvWHn+8/E+GW1TrNZ5j9r+V9GNJr9veVk1brpGQ/9b2YknvS/phRzoEUIumYY+I/5I05uDuki6rtx0AncLPZYEkCDuQBGEHkiDsQBKEHUiCW1zHadq0aQ1ra9euLc47Z86cYn3WrFmttFSLF198sVhftWpVsf7UU08V65999tkx94TOYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkuc5+0UUXFevLli0r1ufNm9ewNmPGjJZ6qsunn37asLZmzZrivHfddVexfvDgwZZ6Qv9hyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaS5zj44ONhWvR07duwo1p988sli/dChQ8V66Z7zffv2FedFHmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T5A/YZkh6SNCApJK2LiH+1fYekWyT9b/XR5RHxuybfVV4YgLZFxJijLo8n7NMlTY+IrbanSHpZ0rUaGY/9k4i4Z7xNEHag8xqFfTzjsw9LGq5eH7D9pqTe/mkWAMfsmI7Zbc+U9D1JW6pJS22/ZvsB26c2mGeJ7SHbQ+21CqAdTXfjv/qg/W1JL0haERGP2h6Q9KFGjuP/SSO7+jc3+Q5244EOa/mYXZJsf0vSk5KeiojVY9RnSnoyIs5v8j2EHeiwRmFvuhtv25Lul/Tm6KBXJ+6OGpS0vd0mAXTOeM7Gz5f0n5Jel3Skmrxc0iJJczSyG/+epJ9WJ/NK38WWHeiwtnbj60LYgc5reTcewMRA2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLbQzZ/KOn9Ue9Pq6b1o37trV/7kuitVXX29peNCl29n/0bC7eHImJuzxoo6Nfe+rUvid5a1a3e2I0HkiDsQBK9Dvu6Hi+/pF9769e+JHprVVd66+kxO4Du6fWWHUCXEHYgiZ6E3fYVtt+y/Y7t23vRQyO237P9uu1tvR6frhpDb4/t7aOmTbP9jO23q+cxx9jrUW932N5Vrbtttq/qUW9n2H7O9g7bb9i+rZre03VX6Ksr663rx+y2J0n6g6QFknZKeknSoojY0dVGGrD9nqS5EdHzH2DY/jtJn0h66OjQWrb/WdLeiFhZ/Ud5akT8Q5/0doeOcRjvDvXWaJjxn6iH667O4c9b0Yst+zxJ70TEuxHxpaTfSFrYgz76XkRslrT3a5MXSlpfvV6vkX8sXdegt74QEcMRsbV6fUDS0WHGe7ruCn11RS/CPkPSH0e936n+Gu89JD1t+2XbS3rdzBgGRg2z9YGkgV42M4amw3h309eGGe+bddfK8Oft4gTdN82PiL+WdKWkW6vd1b4UI8dg/XTt9BeSztLIGIDDklb1splqmPFHJP0sIvaPrvVy3Y3RV1fWWy/CvkvSGaPef6ea1hciYlf1vEfSYxo57Ognu4+OoFs97+lxP1+JiN0RcTgijkj6pXq47qphxh+R9OuIeLSa3PN1N1Zf3VpvvQj7S5LOtv1d2ydK+pGkjT3o4xtsT65OnMj2ZEk/UP8NRb1R0k3V65skPd7DXv5Evwzj3WiYcfV43fV8+POI6PpD0lUaOSP/P5L+sRc9NOhrlqRXq8cbve5N0gaN7Nb9n0bObSyW9OeSnpX0tqT/kDStj3r7N40M7f2aRoI1vUe9zdfILvprkrZVj6t6ve4KfXVlvfFzWSAJTtABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/DyJ7caZa7LphAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0].reshape((28,28)) \n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5491e36",
   "metadata": {},
   "source": [
    "### converting numpy arrays to tensors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65496c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68088c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train[0]))\n",
    "print(x_train[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f61872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier init\n",
    "weights_0 = torch.randn(784, 10) / np.sqrt(784)\n",
    "# record ops who are using the tensor , for back prop \n",
    "weights_0.requires_grad_()\n",
    "biais_0 = torch.zeros(10, requires_grad=True) \n",
    "\n",
    "weights_1 = torch.randn(10,10) / np.sqrt(10) \n",
    "weights_1.requires_grad_() \n",
    "biais_1 = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a94bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def log_softmax(z): \n",
    "    return z - z.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def relu(z): \n",
    "    z[z<0] = 0\n",
    "    return z\n",
    "\n",
    "def model(inputs): \n",
    "    a1 = relu(inputs @ weights_0 + biais_0) \n",
    "    a2 = log_softmax(a1 @ weights_1 + biais_1)\n",
    "    \n",
    "    return a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da577c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5413, -2.5696, -2.2639, -2.5594, -2.2920, -2.1553, -2.5200, -2.0364,\n",
       "        -2.2337, -2.0531], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 12 \n",
    "\n",
    "samples = x_train[:12] \n",
    "preds = model(samples) \n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51ad6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([12, 10])\n"
     ]
    }
   ],
   "source": [
    "y = y_train[:batch_size]\n",
    "print(y.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d032bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(input, target): \n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0f7f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5413], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[[0],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c05c099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3597, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_log_likelihood(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bccaf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true): \n",
    "    cls = torch.argmax(y_pred, dim=1) \n",
    "    return torch.where(y_true == cls, 1, 0).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3cfb60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87503f7b",
   "metadata": {},
   "source": [
    "### writing a training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f47d2e",
   "metadata": {},
   "source": [
    "<p>loss.backward() updates the gradients of the model, in this case, weights and bias.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8742c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 2 \n",
    "nb_batch = (len(x_train)-1) // batch_size + 1\n",
    "\n",
    "for i in range(epochs): \n",
    "    for batch in range(nb_batch): \n",
    "        start = batch * batch_size \n",
    "        end = start + batch_size\n",
    "        \n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end] \n",
    "        \n",
    "        preds = model(x_batch) \n",
    "        loss = negative_log_likelihood(preds, y_train[start:end]) \n",
    "        \n",
    "        # compute loss grad wrt to model parameters\n",
    "        loss.backward() \n",
    "        \n",
    "        with torch.no_grad(): # Context-manager that disabled gradient calculation\n",
    "            # we are cancelling the grad computation when modifying our parameters \n",
    "            weights_0 -= weights_0.grad * lr \n",
    "            biais_0 -= biais_0.grad * lr\n",
    "            weights_1 -= weights_1.grad * lr \n",
    "            biais_1 -= biais_1.grad * lr\n",
    "            \n",
    "            # reset gradients regarding parameters\n",
    "            weights_0.grad.zero_()\n",
    "            weights_1.grad.zero_()\n",
    "            biais_0.grad.zero_()\n",
    "            biais_1.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d786a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.1516, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"loss\", negative_log_likelihood(model(x_train[:12]), y_train[:12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "840919b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9167)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train[:12]), y_train[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722b2a2",
   "metadata": {},
   "source": [
    "### Refactor using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13fad9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "class MnistNet(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / np.sqrt(784))\n",
    "        self.biais = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return x @ self.weights + self.biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "186974b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cdbb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(): \n",
    "    for epoch in range(epochs): \n",
    "        for i in range((x_train.shape[0] -1) // batch_size + 1): \n",
    "            start = i*batch_size\n",
    "            end = i*batch_size + batch_size \n",
    "            \n",
    "            x_batch = x_train[start:end] \n",
    "            y_batch = y_train[start:end] \n",
    "            \n",
    "            pred = model(x_batch) \n",
    "            loss = loss_func(pred, y_batch) \n",
    "            \n",
    "            #backprop\n",
    "            loss.backward() \n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                for p in model.parameters(): \n",
    "                    p -= p.grad * lr\n",
    "                # reset the gradient regarding each parameters\n",
    "                model.zero_grad()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b236b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62a47f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0746, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(x_train[:12]), y_train[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be0128",
   "metadata": {},
   "source": [
    "### Refactoring using linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f031ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear layer does a@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc628dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return x @ self.weights + self.biais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
